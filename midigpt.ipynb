{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLvzrYpdnJBS"
      },
      "outputs": [],
      "source": [
        "%pip install miditok\n",
        "%pip install torchtoolkit\n",
        "%pip install git+https://github.com/huggingface/transformers\n",
        "%pip install git+https://github.com/huggingface/accelerate\n",
        "%pip install git+https://github.com/huggingface/evaluate\n",
        "%pip install torch\n",
        "%pip install tqdm\n",
        "%pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vESBqO6KW4-i"
      },
      "outputs": [],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E84hw2MyLUzQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import json\n",
        "import sys\n",
        "\n",
        "from miditok import REMI\n",
        "from miditok.constants import CHORD_MAPS, ADDITIONAL_TOKENS\n",
        "from miditok.utils import get_midi_programs\n",
        "from miditoolkit import MidiFile\n",
        "from pathlib import Path\n",
        "from torchtoolkit.data import create_subsets\n",
        "from transformers import GPT2LMHeadModel, GPT2Config, Trainer, TrainingArguments, GenerationConfig\n",
        "from evaluate import load as load_metric\n",
        "from typing import Any, Dict, List\n",
        "from torch import Tensor, LongTensor, flip, cat, full, argmax, cuda, no_grad\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "from glob import glob\n",
        "\n",
        "\n",
        "os.environ['WANDB_DISABLED'] = 'false'\n",
        "os.environ[\"WANDB_PROJECT\"]=\"test\"\n",
        "os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
        "os.environ[\"WANDB_WATCH\"]=\"false\"\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
        "\n",
        "is_colab = False\n",
        "is_BPE = True\n",
        "\n",
        "seed = random.randint(1000, 10000)\n",
        "device = \"cuda:0\" if cuda.is_available() else \"cpu\"\n",
        "\n",
        "pitch_range = range(21, 109)\n",
        "additional_tokens = ADDITIONAL_TOKENS\n",
        "additional_tokens['Chord'] = True\n",
        "additional_tokens['TimeSignature'] = True\n",
        "additional_tokens['Rest'] = True\n",
        "tokenizer = REMI(pitch_range=pitch_range, additional_tokens=additional_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuwFhhU-LUzS"
      },
      "outputs": [],
      "source": [
        "len(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scQ1AyYHurzz"
      },
      "outputs": [],
      "source": [
        "path_suffix = '/bpe' if is_BPE else ''\n",
        "\n",
        "if is_colab:\n",
        "  from google.colab import drive\n",
        "  \n",
        "  # drive.mount('/content/drive', force_remount=True)\n",
        "  sys.path.append('/content/drive/MyDrive/colab/python')\n",
        "  from midi_dataset import MIDIDataset, DataCollatorGen\n",
        "\n",
        "  base_path = f'/content/drive/MyDrive/colab{path_suffix}'\n",
        "else:\n",
        "  from utils.midi_dataset import MIDIDataset, DataCollatorGen\n",
        "\n",
        "  base_path = f'/home/nico/data/ai/models/midi{path_suffix}'\n",
        "\n",
        "params_path = Path(f'{base_path}/token_params.json')\n",
        "data_paths = glob(f'{base_path}/130_*.json')\n",
        "\n",
        "list(data_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i50eSqufnJBV"
      },
      "outputs": [],
      "source": [
        "params = tokenizer.load_params(params_path)\n",
        "midi_dataset = MIDIDataset(\n",
        "    files_paths=[Path(path) for path in data_paths],\n",
        "    min_seq_len=24,\n",
        "    max_seq_len=256\n",
        ")\n",
        "subset_train, subset_valid = create_subsets(midi_dataset, [0.3])\n",
        "\n",
        "len(subset_valid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuv56aS3nJBW"
      },
      "outputs": [],
      "source": [
        "cuda.empty_cache()\n",
        "\n",
        "# Creates model\n",
        "config = GPT2Config(\n",
        "    vocab_size=len(tokenizer),\n",
        "    n_positions=2048,\n",
        "    n_embd=1024,\n",
        "    n_layer=16,\n",
        "    n_head=32,\n",
        "    n_inner=2048,\n",
        "    activation_function='gelu_new',\n",
        "    resid_pdrop=.25,\n",
        "    embd_pdrop=.25,\n",
        "    attn_pdrop=.1,\n",
        "    padding_token_id=tokenizer['PAD_None'],\n",
        "    bos_token_id=tokenizer['BOS_None'],\n",
        "    eos_token_id=tokenizer['EOS_None']\n",
        ")\n",
        "\n",
        "with no_grad():\n",
        "    model = GPT2LMHeadModel(config)\n",
        "\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nh69gxEnJBW"
      },
      "outputs": [],
      "source": [
        "metrics = {metric: load_metric(metric) for metric in [\"accuracy\"]}\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Computes metrics for pretraining.\n",
        "    Must use proprocess_logits function that converts logits to predictions (argmax or sampling).\n",
        "\n",
        "    :param eval_pred: EvalPrediction containing predictions and labels\n",
        "    :return: metrics\n",
        "    \"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    not_pad_mask = labels != -100\n",
        "    labels, predictions = labels[not_pad_mask], predictions[not_pad_mask]\n",
        "    computed = metrics[\"accuracy\"].compute(\n",
        "        predictions=predictions.flatten(), references=labels.flatten())\n",
        "\n",
        "    return computed\n",
        "\n",
        "\n",
        "def preprocess_logits(logits: Tensor, _: Tensor) -> Tensor:\n",
        "    \"\"\"Preprocesses the logits before accumulating them during evaluation.\n",
        "    This allows to significantly reduce the memory usage and make the training tractable.\n",
        "    \"\"\"\n",
        "    pred_ids = argmax(logits, dim=-1)  # long dtype\n",
        "    return pred_ids\n",
        "\n",
        "training_config = TrainingArguments(\n",
        "    do_eval=True,\n",
        "    do_predict=False,\n",
        "    do_train=True,\n",
        "    eval_steps=100,\n",
        "    evaluation_strategy='steps',\n",
        "    fp16=True,\n",
        "    greater_is_better=False,\n",
        "    label_smoothing_factor=0.,\n",
        "    learning_rate=1e-5,\n",
        "    load_best_model_at_end=True,\n",
        "    logging_steps=20,\n",
        "    logging_strategy=\"steps\",\n",
        "    log_level=\"debug\",\n",
        "    lr_scheduler_type=\"cosine_with_restarts\",\n",
        "    max_grad_norm=1.5,\n",
        "    max_steps=1000,\n",
        "    metric_for_best_model='loss',\n",
        "    no_cuda=False,\n",
        "    optim=\"adafactor\",\n",
        "    output_dir='./runs',\n",
        "    overwrite_output_dir=False,\n",
        "    per_device_eval_batch_size=4,\n",
        "    per_device_train_batch_size=4,\n",
        "    report_to='wandb',\n",
        "    save_steps=100,\n",
        "    save_strategy=\"steps\",\n",
        "    save_total_limit=5,\n",
        "    seed=seed,\n",
        "    warmup_ratio=0.15,\n",
        "    weight_decay=0.005\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_config,\n",
        "    data_collator=DataCollatorGen(tokenizer[\"PAD_None\"]),\n",
        "    train_dataset=subset_train,\n",
        "    eval_dataset=subset_valid,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=None,\n",
        "    preprocess_logits_for_metrics=preprocess_logits,\n",
        ")\n",
        "\n",
        "# Training\n",
        "train_result = trainer.train()\n",
        "trainer.save_model()  # Saves the tokenizer too\n",
        "trainer.log_metrics(\"train\", train_result.metrics)\n",
        "trainer.save_metrics(\"train\", train_result.metrics)\n",
        "trainer.save_state()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cgz1-gGnJBX"
      },
      "outputs": [],
      "source": [
        "cuda.empty_cache()\n",
        "\n",
        "with no_grad():\n",
        "    model = GPT2LMHeadModel.from_pretrained('./runs/')\n",
        "    model = model.to(device)\n",
        "\n",
        "\n",
        "def collate_gen_left(batch: List[Dict[str, LongTensor]]) -> LongTensor:\n",
        "    # Here the sequences are padded to the left, so that the last token along the time dimension\n",
        "    # is always the last token of each seq, allowing to efficiently generate by batch\n",
        "    bos_shape = (1,)\n",
        "    batch = [flip(cat([full(bos_shape, tokenizer[\"BOS_None\"]),\n",
        "                  seq[\"input_ids\"]], dim=0), dims=(0,)) for seq in batch]\n",
        "    batch = pad_sequence(batch, batch_first=True,\n",
        "                         padding_value=tokenizer[\"PAD_None\"])  # (N,T) or (N,T,Z)\n",
        "    batch = flip(batch, dims=(1,)).long()\n",
        "    return batch  # (N,T)\n",
        "\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    max_new_tokens=512,  # extends samples by 512 tokens\n",
        "    num_beams=1,        # no beam search\n",
        "    do_sample=True,     # but sample instead\n",
        "    temperature=0.35,\n",
        "    top_k=15,\n",
        "    top_p=0.25,\n",
        "    epsilon_cutoff=3e-4,\n",
        "    eta_cutoff=1e-3,\n",
        "    pad_token_id=config.padding_token_id,\n",
        "    penalty_alpha=1.5,\n",
        "    repetition_penalty=1.5\n",
        ")\n",
        "\n",
        "(gen_results_path := Path('gen_res')).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "def rec_gen(tokens):\n",
        "    global model\n",
        "    global generation_config\n",
        "\n",
        "    res = model.generate(LongTensor([tokens]).to(model.device),\n",
        "                         generation_config=generation_config)\n",
        "\n",
        "    out = res[0].cpu().numpy().tolist()\n",
        "    new_tokens = out[len(tokens)-1:]\n",
        "\n",
        "    print(f'Generated {len(new_tokens)} new tokens.')\n",
        "\n",
        "    return new_tokens\n",
        "\n",
        "\n",
        "max_iter = 2\n",
        "iter_count = 0\n",
        "init_size = 256\n",
        "\n",
        "with open(f'{base_path}/ozzy_osbourne-facing_hell.json') as tokens_file:\n",
        "    ids = json.load(tokens_file)['ids']\n",
        "    tokens = ids[0][:init_size]  # 1 channel only\n",
        "\n",
        "    while iter_count < max_iter:\n",
        "        block_size = init_size if iter_count == 0 else int(init_size / 128)\n",
        "        tokens += rec_gen(tokens[-block_size:])\n",
        "\n",
        "        iter_count += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6O_JcPggnJBZ"
      },
      "outputs": [],
      "source": [
        "print('Generating the midi...')\n",
        "\n",
        "midi = tokenizer.tokens_to_midi(LongTensor([tokens]).cpu(), time_division=384)\n",
        "# midi.instruments[0].name = f'Continuation of original sample ({len(generated)} tokens)'\n",
        "# midi.instruments[1].name = f'Original sample ({len(prompt)} tokens)'\n",
        "# midi.instruments[2].name = f'Original sample and continuation'\n",
        "midi.dump(gen_results_path / 'full.mid')\n",
        "# tokenizer.save_tokens(tokens, gen_results_path / f'{count}.json')\n",
        "\n",
        "type(midi)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "102f73d77db8f9e0c75b800ffd71173bec971c7aa34b8f73fca7d4247b616f8e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
