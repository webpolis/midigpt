{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from miditok import REMI\n",
    "from miditok.utils import get_midi_programs\n",
    "from miditoolkit import MidiFile\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "\n",
    "seed = random.randint(1000, 10000)\n",
    "tokenizer = REMI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: /home/nico/data/ai/models/midi: 100%|██████████| 9/9 [00:00<00:00, 287.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtoolkit.data import create_subsets\n",
    "from utils.midi_dataset import MIDIDataset\n",
    "\n",
    "#tokens = tokenizer.load_tokens(Path('/home/nico/data/ai/models/midi/tokens.json'))\n",
    "params = tokenizer.load_params(\n",
    "    Path('/home/nico/data/ai/models/midi/token_params.json'))\n",
    "\n",
    "midi_dataset = MIDIDataset(\n",
    "    files_paths=list(\n",
    "        Path('/home/nico/data/ai/models/midi/').glob('ozzy*.json')),\n",
    "    min_seq_len=24,\n",
    "    max_seq_len=128\n",
    ")\n",
    "subset_train, subset_valid = create_subsets(midi_dataset, [0.3])\n",
    "\n",
    "len(subset_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(221, 512)\n",
       "    (wpe): Embedding(2048, 512)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-7): 8 x GPT2Block(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=221, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Config, Trainer, TrainingArguments, GenerationConfig\n",
    "\n",
    "# Creates model\n",
    "config = GPT2Config(\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_positions=2048,\n",
    "    n_embd=512,\n",
    "    n_layer=8,\n",
    "    n_head=8,\n",
    "    n_inner=2048,\n",
    "    resid_pdrop=.1,\n",
    "    embd_pdrop=.1,\n",
    "    attn_pdrop=.1,\n",
    "    padding_token_id=tokenizer['PAD_None'],\n",
    "    bos_token_id=tokenizer['BOS_None'],\n",
    "    eos_token_id=tokenizer['EOS_None']\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using cuda_amp half precision backend\n",
      "/home/nico/anaconda3/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 145\n",
      "  Num Epochs = 28\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1,000\n",
      "  Number of trainable parameters = 26,381,824\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc70a9c93e574af987e2218f6c916bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.2089, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.55}\n",
      "{'loss': 3.9942, 'learning_rate': 1.3333333333333333e-05, 'epoch': 1.1}\n",
      "{'loss': 3.2065, 'learning_rate': 2e-05, 'epoch': 1.64}\n",
      "{'loss': 2.7077, 'learning_rate': 2.6666666666666667e-05, 'epoch': 2.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 62\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2786, 'learning_rate': 3.3333333333333335e-05, 'epoch': 2.74}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6199b61a75f04f75b3cf98429bffcd8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to runs/checkpoint-100\n",
      "Configuration saved in runs/checkpoint-100/config.json\n",
      "Configuration saved in runs/checkpoint-100/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.044809579849243, 'eval_accuracy': 0.0068319838056680165, 'eval_runtime': 1.489, 'eval_samples_per_second': 41.639, 'eval_steps_per_second': 20.819, 'epoch': 2.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in runs/checkpoint-100/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0788, 'learning_rate': 4e-05, 'epoch': 3.29}\n",
      "{'loss': 1.9711, 'learning_rate': 4.666666666666667e-05, 'epoch': 3.84}\n",
      "{'loss': 1.8444, 'learning_rate': 5.333333333333333e-05, 'epoch': 4.38}\n",
      "{'loss': 1.7867, 'learning_rate': 6e-05, 'epoch': 4.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 62\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6621, 'learning_rate': 6.666666666666667e-05, 'epoch': 5.48}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb93fdacb304670844af7aa9a8d5127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to runs/checkpoint-200\n",
      "Configuration saved in runs/checkpoint-200/config.json\n",
      "Configuration saved in runs/checkpoint-200/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6472399234771729, 'eval_accuracy': 0.001771255060728745, 'eval_runtime': 1.4467, 'eval_samples_per_second': 42.856, 'eval_steps_per_second': 21.428, 'epoch': 5.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in runs/checkpoint-200/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.57, 'learning_rate': 7.333333333333333e-05, 'epoch': 6.03}\n",
      "{'loss': 1.5773, 'learning_rate': 8e-05, 'epoch': 6.58}\n",
      "{'loss': 1.5249, 'learning_rate': 8.666666666666667e-05, 'epoch': 7.12}\n",
      "{'loss': 1.3723, 'learning_rate': 9.333333333333334e-05, 'epoch': 7.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 62\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4194, 'learning_rate': 0.0001, 'epoch': 8.22}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ed8f7836d04a0ca6d98c3760aaee59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to runs/checkpoint-300\n",
      "Configuration saved in runs/checkpoint-300/config.json\n",
      "Configuration saved in runs/checkpoint-300/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4831056594848633, 'eval_accuracy': 0.0036690283400809716, 'eval_runtime': 1.4, 'eval_samples_per_second': 44.285, 'eval_steps_per_second': 22.142, 'epoch': 8.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in runs/checkpoint-300/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3373, 'learning_rate': 9.979871469976196e-05, 'epoch': 8.77}\n",
      "{'loss': 1.3088, 'learning_rate': 9.919647942993148e-05, 'epoch': 9.32}\n",
      "{'loss': 1.1787, 'learning_rate': 9.819814303479267e-05, 'epoch': 9.86}\n",
      "{'loss': 1.0688, 'learning_rate': 9.681174353198687e-05, 'epoch': 10.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 62\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1103, 'learning_rate': 9.504844339512095e-05, 'epoch': 10.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272cfc7c29334321b5300a36be6ac20e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to runs/checkpoint-400\n",
      "Configuration saved in runs/checkpoint-400/config.json\n",
      "Configuration saved in runs/checkpoint-400/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3541311025619507, 'eval_accuracy': 0.005060728744939271, 'eval_runtime': 1.4476, 'eval_samples_per_second': 42.831, 'eval_steps_per_second': 21.415, 'epoch': 10.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in runs/checkpoint-400/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9992, 'learning_rate': 9.292243968009331e-05, 'epoch': 11.51}\n",
      "{'loss': 1.0008, 'learning_rate': 9.045084971874738e-05, 'epoch': 12.05}\n",
      "{'loss': 0.8568, 'learning_rate': 8.765357330018056e-05, 'epoch': 12.6}\n",
      "{'loss': 0.8321, 'learning_rate': 8.455313244934324e-05, 'epoch': 13.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 62\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8216, 'learning_rate': 8.117449009293668e-05, 'epoch': 13.7}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a031f46e86bc4b2cb00fb989588baca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to runs/checkpoint-500\n",
      "Configuration saved in runs/checkpoint-500/config.json\n",
      "Configuration saved in runs/checkpoint-500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2271618843078613, 'eval_accuracy': 0.008097165991902834, 'eval_runtime': 1.4831, 'eval_samples_per_second': 41.803, 'eval_steps_per_second': 20.902, 'epoch': 13.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in runs/checkpoint-500/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7279, 'learning_rate': 7.754484907260513e-05, 'epoch': 14.25}\n",
      "{'loss': 0.6987, 'learning_rate': 7.369343312364993e-05, 'epoch': 14.79}\n",
      "{'loss': 0.6533, 'learning_rate': 6.965125158269619e-05, 'epoch': 15.34}\n",
      "{'loss': 0.6383, 'learning_rate': 6.545084971874738e-05, 'epoch': 15.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 62\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5569, 'learning_rate': 6.112604669781572e-05, 'epoch': 16.44}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2146c6eff607446d9269f4ab9cd8c13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to runs/checkpoint-600\n",
      "Configuration saved in runs/checkpoint-600/config.json\n",
      "Configuration saved in runs/checkpoint-600/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1665607690811157, 'eval_accuracy': 0.005819838056680162, 'eval_runtime': 1.3023, 'eval_samples_per_second': 47.607, 'eval_steps_per_second': 23.803, 'epoch': 16.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in runs/checkpoint-600/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-100] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5219, 'learning_rate': 5.6711663290882776e-05, 'epoch': 16.99}\n",
      "{'loss': 0.4473, 'learning_rate': 5.2243241517525754e-05, 'epoch': 17.53}\n",
      "{'loss': 0.4815, 'learning_rate': 4.775675848247427e-05, 'epoch': 18.08}\n",
      "{'loss': 0.4056, 'learning_rate': 4.328833670911724e-05, 'epoch': 18.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 62\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4161, 'learning_rate': 3.887395330218429e-05, 'epoch': 19.18}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b80e97cc10d492eb0713c49db7ce030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to runs/checkpoint-700\n",
      "Configuration saved in runs/checkpoint-700/config.json\n",
      "Configuration saved in runs/checkpoint-700/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1532626152038574, 'eval_accuracy': 0.010880566801619434, 'eval_runtime': 1.4545, 'eval_samples_per_second': 42.627, 'eval_steps_per_second': 21.313, 'epoch': 19.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in runs/checkpoint-700/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-200] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3645, 'learning_rate': 3.4549150281252636e-05, 'epoch': 19.73}\n",
      "{'loss': 0.3144, 'learning_rate': 3.0348748417303823e-05, 'epoch': 20.27}\n",
      "{'loss': 0.3242, 'learning_rate': 2.630656687635007e-05, 'epoch': 20.82}\n",
      "{'loss': 0.2809, 'learning_rate': 2.245515092739488e-05, 'epoch': 21.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 62\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2769, 'learning_rate': 1.8825509907063327e-05, 'epoch': 21.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f60be626994191a5bc9a0657908098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to runs/checkpoint-800\n",
      "Configuration saved in runs/checkpoint-800/config.json\n",
      "Configuration saved in runs/checkpoint-800/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1487267017364502, 'eval_accuracy': 0.008223684210526315, 'eval_runtime': 1.5001, 'eval_samples_per_second': 41.332, 'eval_steps_per_second': 20.666, 'epoch': 21.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in runs/checkpoint-800/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-300] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2538, 'learning_rate': 1.544686755065677e-05, 'epoch': 22.47}\n",
      "{'loss': 0.2584, 'learning_rate': 1.2346426699819458e-05, 'epoch': 23.01}\n",
      "{'loss': 0.2249, 'learning_rate': 9.549150281252633e-06, 'epoch': 23.56}\n",
      "{'loss': 0.2179, 'learning_rate': 7.077560319906695e-06, 'epoch': 24.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 62\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2109, 'learning_rate': 4.951556604879048e-06, 'epoch': 24.66}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8371fa96c80d4d01a6fec0cf147122f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to runs/checkpoint-900\n",
      "Configuration saved in runs/checkpoint-900/config.json\n",
      "Configuration saved in runs/checkpoint-900/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1691257953643799, 'eval_accuracy': 0.008350202429149798, 'eval_runtime': 1.4726, 'eval_samples_per_second': 42.104, 'eval_steps_per_second': 21.052, 'epoch': 24.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in runs/checkpoint-900/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-400] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.221, 'learning_rate': 3.18825646801314e-06, 'epoch': 25.21}\n",
      "{'loss': 0.2168, 'learning_rate': 1.8018569652073381e-06, 'epoch': 25.75}\n",
      "{'loss': 0.2076, 'learning_rate': 8.035205700685167e-07, 'epoch': 26.3}\n",
      "{'loss': 0.184, 'learning_rate': 2.012853002380466e-07, 'epoch': 26.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 62\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2121, 'learning_rate': 0.0, 'epoch': 27.4}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c29f8e6aecd48abade3c3279eb4892a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to runs/checkpoint-1000\n",
      "Configuration saved in runs/checkpoint-1000/config.json\n",
      "Configuration saved in runs/checkpoint-1000/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.173716425895691, 'eval_accuracy': 0.00847672064777328, 'eval_runtime': 1.4361, 'eval_samples_per_second': 43.174, 'eval_steps_per_second': 21.587, 'epoch': 27.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in runs/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from runs/checkpoint-700 (score: 0.010880566801619434).\n",
      "Saving model checkpoint to runs\n",
      "Configuration saved in runs/config.json\n",
      "Configuration saved in runs/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 304.041, 'train_samples_per_second': 13.156, 'train_steps_per_second': 3.289, 'train_loss': 1.0806627798080444, 'epoch': 27.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in runs/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       27.4\n",
      "  train_loss               =     1.0807\n",
      "  train_runtime            = 0:05:04.04\n",
      "  train_samples_per_second =     13.156\n",
      "  train_steps_per_second   =      3.289\n"
     ]
    }
   ],
   "source": [
    "from utils.midi_dataset import DataCollatorGen\n",
    "from evaluate import load as load_metric\n",
    "from torch import Tensor, argmax\n",
    "\n",
    "metrics = {metric: load_metric(metric) for metric in [\"accuracy\"]}\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes metrics for pretraining.\n",
    "    Must use proprocess_logits function that converts logits to predictions (argmax or sampling).\n",
    "\n",
    "    :param eval_pred: EvalPrediction containing predictions and labels\n",
    "    :return: metrics\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    not_pad_mask = labels != -100\n",
    "    labels, predictions = labels[not_pad_mask], predictions[not_pad_mask]\n",
    "    computed = metrics[\"accuracy\"].compute(\n",
    "        predictions=predictions.flatten(), references=labels.flatten())\n",
    "\n",
    "    return computed\n",
    "\n",
    "\n",
    "def preprocess_logits(logits: Tensor, _: Tensor) -> Tensor:\n",
    "    \"\"\"Preprocesses the logits before accumulating them during evaluation.\n",
    "    This allows to significantly reduce the memory usage and make the training tractable.\n",
    "    \"\"\"\n",
    "    pred_ids = argmax(logits, dim=-1)  # long dtype\n",
    "    return pred_ids\n",
    "\n",
    "\n",
    "training_config = TrainingArguments(\n",
    "    \"runs\", False, True, True, False, \"steps\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    eval_accumulation_steps=2,\n",
    "    eval_steps=100,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=3.0,\n",
    "    max_steps=1000,\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    warmup_ratio=0.3,\n",
    "    log_level=\"debug\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"steps\",\n",
    "    metric_for_best_model='accuracy',\n",
    "    greater_is_better=True,\n",
    "    save_steps=100,\n",
    "    save_total_limit=5,\n",
    "    no_cuda=False,\n",
    "    seed=seed,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    label_smoothing_factor=0.,\n",
    "    optim=\"adamw_hf\",\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_num_workers=2,\n",
    "    auto_find_batch_size=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_config,\n",
    "    data_collator=DataCollatorGen(tokenizer[\"PAD_None\"]),\n",
    "    train_dataset=subset_train,\n",
    "    eval_dataset=subset_valid,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=None,\n",
    "    preprocess_logits_for_metrics=preprocess_logits,\n",
    ")\n",
    "\n",
    "# Training\n",
    "train_result = trainer.train()\n",
    "trainer.save_model()  # Saves the tokenizer too\n",
    "trainer.log_metrics(\"train\", train_result.metrics)\n",
    "trainer.save_metrics(\"train\", train_result.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./runs/config.json\n",
      "Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_embd\": 512,\n",
      "  \"n_head\": 8,\n",
      "  \"n_inner\": 2048,\n",
      "  \"n_layer\": 8,\n",
      "  \"n_positions\": 2048,\n",
      "  \"padding_token_id\": 0,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.29.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 221\n",
      "}\n",
      "\n",
      "loading weights file ./runs/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"transformers_version\": \"4.29.0.dev0\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./runs/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "loading configuration file ./runs/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"transformers_version\": \"4.29.0.dev0\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">74</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">71 │   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">72 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">while</span> iter_count &lt; max_iter:                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">73 │   │   </span>block_size = init_size <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> iter_count == <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">int</span>(init_size / <span style=\"color: #0000ff; text-decoration-color: #0000ff\">128</span>)                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>74 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>tokens += rec_gen(tokens[-block_size:])                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">75 │   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">76 │   │   </span>iter_count += <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">77 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">rec_gen</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">53</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">50 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">global</span> model                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">51 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">global</span> generation_config                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">52 │   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>53 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>res = model.generate(torch.LongTensor([tokens]).to(model.device),                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">54 │   │   │   │   │   │    </span>generation_config=generation_config)                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">55 │   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">56 │   </span>out = res[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>].cpu().numpy().tolist()                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/nico/anaconda3/lib/python3.10/site-packages/torch/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_contextlib.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">115</span> in              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decorate_context</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112 │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">@functools</span>.wraps(func)                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">113 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decorate_context</span>(*args, **kwargs):                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">114 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> ctx_factory():                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>115 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> func(*args, **kwargs)                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">116 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">117 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> decorate_context                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">118 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/nico/anaconda3/lib/python3.10/site-packages/transformers/generation/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1601</span> in       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generate</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1598 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1599 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> is_group_beam_gen_mode:                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1600 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> generation_config.num_return_sequences &gt; generation_config.num_beams:      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1601 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"`num_return_sequences` has to be smaller or equal to `</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1602 │   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1603 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> generation_config.num_beams % generation_config.num_beam_groups != <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>:      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1604 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"`num_beams` should be divisible by `num_beam_groups` f</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValueError: </span>`num_return_sequences` has to be smaller or equal to `num_beams`.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m74\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m71 \u001b[0m\u001b[2m│   \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m72 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mwhile\u001b[0m iter_count < max_iter:                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m73 \u001b[0m\u001b[2m│   │   \u001b[0mblock_size = init_size \u001b[94mif\u001b[0m iter_count == \u001b[94m0\u001b[0m \u001b[94melse\u001b[0m \u001b[96mint\u001b[0m(init_size / \u001b[94m128\u001b[0m)                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m74 \u001b[2m│   │   \u001b[0mtokens += rec_gen(tokens[-block_size:])                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m75 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m76 \u001b[0m\u001b[2m│   │   \u001b[0miter_count += \u001b[94m1\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m77 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mrec_gen\u001b[0m:\u001b[94m53\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m50 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mglobal\u001b[0m model                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m51 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mglobal\u001b[0m generation_config                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m52 \u001b[0m\u001b[2m│   \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m53 \u001b[2m│   \u001b[0mres = model.generate(torch.LongTensor([tokens]).to(model.device),                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m54 \u001b[0m\u001b[2m│   │   │   │   │   │    \u001b[0mgeneration_config=generation_config)                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m55 \u001b[0m\u001b[2m│   \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m56 \u001b[0m\u001b[2m│   \u001b[0mout = res[\u001b[94m0\u001b[0m].cpu().numpy().tolist()                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/nico/anaconda3/lib/python3.10/site-packages/torch/utils/\u001b[0m\u001b[1;33m_contextlib.py\u001b[0m:\u001b[94m115\u001b[0m in              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mdecorate_context\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m│   \u001b[0m\u001b[1;95m@functools\u001b[0m.wraps(func)                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m113 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mdecorate_context\u001b[0m(*args, **kwargs):                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m114 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m ctx_factory():                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m115 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m func(*args, **kwargs)                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m117 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m decorate_context                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m118 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/nico/anaconda3/lib/python3.10/site-packages/transformers/generation/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m1601\u001b[0m in       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mgenerate\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1598 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1599 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m is_group_beam_gen_mode:                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1600 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m generation_config.num_return_sequences > generation_config.num_beams:      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1601 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33m`num_return_sequences` has to be smaller or equal to `\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1602 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1603 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m generation_config.num_beams % generation_config.num_beam_groups != \u001b[94m0\u001b[0m:      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1604 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33m`num_beams` should be divisible by `num_beam_groups` f\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mValueError: \u001b[0m`num_return_sequences` has to be smaller or equal to `num_beams`.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from typing import Any, Dict, List\n",
    "from torch import LongTensor, flip, cat, full\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from transformers import GPT2LMHeadModel, GPT2Config, Trainer, TrainingArguments, GenerationConfig\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = GPT2LMHeadModel.from_pretrained('./runs/')\n",
    "model = model.to(device)\n",
    "\n",
    "def collate_gen_left(batch: List[Dict[str, LongTensor]]) -> LongTensor:\n",
    "    # Here the sequences are padded to the left, so that the last token along the time dimension\n",
    "    # is always the last token of each seq, allowing to efficiently generate by batch\n",
    "    bos_shape = (1,)\n",
    "    batch = [flip(cat([full(bos_shape, tokenizer[\"BOS_None\"]),\n",
    "                  seq[\"input_ids\"]], dim=0), dims=(0,)) for seq in batch]\n",
    "    batch = pad_sequence(batch, batch_first=True,\n",
    "                         padding_value=tokenizer[\"PAD_None\"])  # (N,T) or (N,T,Z)\n",
    "    batch = flip(batch, dims=(1,)).long()\n",
    "    return batch  # (N,T)\n",
    "\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    min_new_tokens=512,\n",
    "    max_new_tokens=1024,\n",
    "    repetition_penalty=1.5,\n",
    "    num_beams=24,        # no beam search\n",
    "    early_stopping=True,\n",
    "    no_repeat_ngram_size=5,\n",
    "    num_return_sequences=16,\n",
    "    length_penalty=0.5,\n",
    "    num_beam_groups=4,\n",
    "    diversity_penalty=2.0,\n",
    "    do_sample=False,     # but sample instead\n",
    "    temperature=0.75,\n",
    "    top_k=35,\n",
    "    top_p=0.35,\n",
    "    epsilon_cutoff=3e-4,\n",
    "    eta_cutoff=1e-3,\n",
    "    pad_token_id=config.padding_token_id,\n",
    ")\n",
    "\n",
    "(gen_results_path := Path('gen_res')).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def rec_gen(tokens):\n",
    "    global model\n",
    "    global generation_config\n",
    "\n",
    "    res = model.generate(torch.LongTensor([tokens]).to(model.device),\n",
    "                         generation_config=generation_config)\n",
    "    \n",
    "    out = res[0].cpu().numpy().tolist()\n",
    "    new_tokens = out[len(tokens)-1:]\n",
    "\n",
    "    print(f'Generated {len(new_tokens)} new tokens.')\n",
    "\n",
    "    return new_tokens\n",
    "\n",
    "\n",
    "max_iter = 2\n",
    "iter_count = 0\n",
    "init_size = 256\n",
    "\n",
    "with open('/home/nico/data/ai/models/midi/ozzy_osbourne-facing_hell.json') as tokens_file:\n",
    "    ids = json.load(tokens_file)['ids']\n",
    "    tokens = ids[0][:init_size] # 1 channel only\n",
    "\n",
    "    while iter_count < max_iter:\n",
    "        block_size = init_size if iter_count == 0 else int(init_size / 128)\n",
    "        tokens += rec_gen(tokens[-block_size:])\n",
    "\n",
    "        iter_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the midi...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "miditoolkit.midi.parser.MidiFile"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print('Generating the midi...')\n",
    "\n",
    "midi = tokenizer.tokens_to_midi(torch.LongTensor([tokens]).cpu(), time_division=384)\n",
    "# midi.instruments[0].name = f'Continuation of original sample ({len(generated)} tokens)'\n",
    "# midi.instruments[1].name = f'Original sample ({len(prompt)} tokens)'\n",
    "# midi.instruments[2].name = f'Original sample and continuation'\n",
    "midi.dump(gen_results_path / 'full.mid')\n",
    "# tokenizer.save_tokens(tokens, gen_results_path / f'{count}.json')\n",
    "\n",
    "type(midi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "102f73d77db8f9e0c75b800ffd71173bec971c7aa34b8f73fca7d4247b616f8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
